{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 actions\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} actions\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # 4 parameters in the observation space\n",
    "        self.actor = nn.Linear(128, 2) # 2 possible actions\n",
    "        self.critic = nn.Linear(128, 1) # Single action value\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_prob, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "# Decide whether to move block left or right based on model output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    # Calculating losses and performing backprop\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    returns = []\n",
    "\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + 0.99 * R # Gamma is 0.99\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = 10\n",
    "    for i_episode in count():\n",
    "        state, info = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):\n",
    "            action = select_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f\"Episode {i_episode} Reward: {ep_reward:.2f} Average reward: {running_reward:.2f}\")\n",
    "\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(f\"Solved, running reward is now {running_reward} and the last episode runs to {t} timesteps\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 12.00 Average reward: 10.10\n",
      "Episode 10 Reward: 29.00 Average reward: 21.14\n",
      "Episode 20 Reward: 66.00 Average reward: 37.12\n",
      "Episode 30 Reward: 16.00 Average reward: 43.68\n",
      "Episode 40 Reward: 16.00 Average reward: 32.50\n",
      "Episode 50 Reward: 33.00 Average reward: 29.84\n",
      "Episode 60 Reward: 63.00 Average reward: 36.02\n",
      "Episode 70 Reward: 95.00 Average reward: 77.25\n",
      "Episode 80 Reward: 34.00 Average reward: 60.08\n",
      "Episode 90 Reward: 44.00 Average reward: 67.65\n",
      "Episode 100 Reward: 26.00 Average reward: 55.39\n",
      "Episode 110 Reward: 28.00 Average reward: 44.77\n",
      "Episode 120 Reward: 288.00 Average reward: 52.37\n",
      "Episode 130 Reward: 146.00 Average reward: 64.60\n",
      "Episode 140 Reward: 189.00 Average reward: 96.45\n",
      "Episode 150 Reward: 176.00 Average reward: 106.07\n",
      "Episode 160 Reward: 180.00 Average reward: 135.01\n",
      "Episode 170 Reward: 81.00 Average reward: 143.93\n",
      "Episode 180 Reward: 270.00 Average reward: 143.04\n",
      "Episode 190 Reward: 89.00 Average reward: 144.53\n",
      "Episode 200 Reward: 63.00 Average reward: 142.95\n",
      "Episode 210 Reward: 117.00 Average reward: 123.16\n",
      "Episode 220 Reward: 104.00 Average reward: 129.21\n",
      "Episode 230 Reward: 148.00 Average reward: 139.97\n",
      "Episode 240 Reward: 190.00 Average reward: 152.45\n",
      "Episode 250 Reward: 109.00 Average reward: 154.24\n",
      "Episode 260 Reward: 139.00 Average reward: 148.54\n",
      "Episode 270 Reward: 240.00 Average reward: 180.28\n",
      "Episode 280 Reward: 500.00 Average reward: 304.16\n",
      "Episode 290 Reward: 342.00 Average reward: 374.84\n",
      "Episode 300 Reward: 500.00 Average reward: 425.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ep_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[1;32m      5\u001b[0m action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39msaved_actions\u001b[38;5;241m.\u001b[39mappend(SavedAction(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, state_value))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/datascience-3.12.5/lib/python3.12/site-packages/torch/distributions/categorical.py:141\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m)\n\u001b[1;32m    142\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_pmf\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, value)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/datascience-3.12.5/lib/python3.12/site-packages/torch/distributions/utils.py:149\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 149\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, value)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/.pyenv/versions/datascience-3.12.5/lib/python3.12/site-packages/torch/distributions/categorical.py:98\u001b[0m, in \u001b[0;36mCategorical.logits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@lazy_property\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogits\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprobs_to_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/datascience-3.12.5/lib/python3.12/site-packages/torch/distributions/utils.py:127\u001b[0m, in \u001b[0;36mprobs_to_logits\u001b[0;34m(probs, is_binary)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobs_to_logits\u001b[39m(probs, is_binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    Converts a tensor of probabilities into logits. For the binary case,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    this denotes the probability of occurrence of the event indexed by `1`.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    For the multi-dimensional case, the values along the last dimension\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    denote the probabilities of occurrence of each of the events.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     ps_clamped \u001b[38;5;241m=\u001b[39m \u001b[43mclamp_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_binary:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlog(ps_clamped) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog1p(\u001b[38;5;241m-\u001b[39mps_clamped)\n",
      "File \u001b[0;32m~/.pyenv/versions/datascience-3.12.5/lib/python3.12/site-packages/torch/distributions/utils.py:117\u001b[0m, in \u001b[0;36mclamp_probs\u001b[0;34m(probs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Clamps the probabilities to be in the open interval `(0, 1)`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mThe probabilities would be clamped between `eps` and `1 - eps`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(probs\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
