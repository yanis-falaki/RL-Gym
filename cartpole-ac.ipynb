{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1') # We make the Cartpole environment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 actions\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} actions\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # 4 parameters in the observation space\n",
    "        self.actor = nn.Linear(128, 2) # 2 possible actions\n",
    "        self.critic = nn.Linear(128, 1) # Single action value\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_prob, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "# Decide whether to move block left or right based on model output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    # Calculating losses and performing backprop\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    returns = []\n",
    "\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + 0.99 * R # Gamma is 0.99\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = 10\n",
    "    for i_episode in count():\n",
    "        state, info = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):\n",
    "            action = select_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f\"Episode {i_episode} Reward: {ep_reward:.2f} Average reward: {running_reward:.2f}\")\n",
    "\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(f\"Solved, running reward is now {running_reward} and the last episode runs to {t} timesteps\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 22.00 Average reward: 10.60\n",
      "Episode 10 Reward: 18.00 Average reward: 13.06\n",
      "Episode 20 Reward: 9.00 Average reward: 12.06\n",
      "Episode 30 Reward: 14.00 Average reward: 11.98\n",
      "Episode 40 Reward: 21.00 Average reward: 12.51\n",
      "Episode 50 Reward: 213.00 Average reward: 31.72\n",
      "Episode 60 Reward: 62.00 Average reward: 56.30\n",
      "Episode 70 Reward: 30.00 Average reward: 74.57\n",
      "Episode 80 Reward: 135.00 Average reward: 95.57\n",
      "Episode 90 Reward: 151.00 Average reward: 109.42\n",
      "Episode 100 Reward: 13.00 Average reward: 127.43\n",
      "Episode 110 Reward: 144.00 Average reward: 140.25\n",
      "Episode 120 Reward: 56.00 Average reward: 145.25\n",
      "Episode 130 Reward: 108.00 Average reward: 119.62\n",
      "Episode 140 Reward: 500.00 Average reward: 149.30\n",
      "Episode 150 Reward: 134.00 Average reward: 197.46\n",
      "Episode 160 Reward: 500.00 Average reward: 283.65\n",
      "Episode 170 Reward: 49.00 Average reward: 269.16\n",
      "Episode 180 Reward: 98.00 Average reward: 286.02\n",
      "Episode 190 Reward: 116.00 Average reward: 278.67\n",
      "Episode 200 Reward: 339.00 Average reward: 258.98\n",
      "Episode 210 Reward: 81.00 Average reward: 315.74\n",
      "Episode 220 Reward: 132.00 Average reward: 234.51\n",
      "Episode 230 Reward: 15.00 Average reward: 222.38\n",
      "Episode 240 Reward: 106.00 Average reward: 171.71\n",
      "Episode 250 Reward: 117.00 Average reward: 150.49\n",
      "Episode 260 Reward: 132.00 Average reward: 141.53\n",
      "Episode 270 Reward: 144.00 Average reward: 141.41\n",
      "Episode 280 Reward: 179.00 Average reward: 144.72\n",
      "Episode 290 Reward: 189.00 Average reward: 154.85\n",
      "Episode 300 Reward: 195.00 Average reward: 165.64\n",
      "Episode 310 Reward: 212.00 Average reward: 184.96\n",
      "Episode 320 Reward: 270.00 Average reward: 218.71\n",
      "Episode 330 Reward: 500.00 Average reward: 297.07\n",
      "Episode 340 Reward: 227.00 Average reward: 349.40\n",
      "Episode 350 Reward: 500.00 Average reward: 375.53\n",
      "Episode 360 Reward: 198.00 Average reward: 349.77\n",
      "Episode 370 Reward: 67.00 Average reward: 261.19\n",
      "Episode 380 Reward: 68.00 Average reward: 183.19\n",
      "Episode 390 Reward: 57.00 Average reward: 133.81\n",
      "Episode 400 Reward: 176.00 Average reward: 106.89\n",
      "Episode 410 Reward: 500.00 Average reward: 146.02\n",
      "Episode 420 Reward: 500.00 Average reward: 260.80\n",
      "Episode 430 Reward: 500.00 Average reward: 347.24\n",
      "Episode 440 Reward: 500.00 Average reward: 408.54\n",
      "Episode 450 Reward: 305.00 Average reward: 435.49\n",
      "Episode 460 Reward: 500.00 Average reward: 430.41\n",
      "Episode 470 Reward: 500.00 Average reward: 458.34\n",
      "Episode 480 Reward: 500.00 Average reward: 475.05\n",
      "Solved, running reward is now 475.05446260698005 and the last episode runs to 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
