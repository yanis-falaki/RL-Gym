{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDictModule(\n",
      "    module=LazyLinear(in_features=0, out_features=1, bias=True),\n",
      "    device=cpu,\n",
      "    in_keys=['observation'],\n",
      "    out_keys=['action'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from pendulum_wrapper import TorchRLPendulumEnv\n",
    "\n",
    "env = TorchRLPendulumEnv(device='cpu')\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialized Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import Actor\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10,  policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "backbone = MLP(in_features=3, out_features=2)\n",
    "extractor = NormalParamExtractor()\n",
    "module = torch.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True\n",
    ")\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set whether or not the policy is deterministic by taking the expected value (`loc` directly when it comes to Normals), or probablistic by sampling randomely from the distribution modeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "    # takes mean as action\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # samples actions according to the dist\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.modules import EGreedyModule\n",
    "\n",
    "policy = Actor(MLP(3, 1, num_cells=[32, 64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_module = EGreedyModule(\n",
    "    spec=env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build an exploratative policy (if not already inherently one) is to concatenate to the deterministic policy module an exploration module within a `TensorDictSequential`. (Which is analoguous to `Sequential` in the tensordict realm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
    "\n",
    "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "    # Turns off exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Turns on exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it must be able to sample random actions in the action space, the EGreedyModule must be equipped with the action_space from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Value Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action-Value network which produces one value per action when it reads the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env needs to have a discrete finite action space\n",
    "# GymEnv is not supported at the moment with gymnasium version >= 1.0, so these blocks shouldn't actually be ran\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\")\n",
    "\n",
    "num_actions = 2\n",
    "value_net = TensorDictModule(\n",
    "    MLP(out_features=num_actions, num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_values\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a Q-Value actor by adding a `QValueModule` after the value network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import QValueModule\n",
    "\n",
    "policy = TensorDictSequential(\n",
    "    value_net, # writes action values into our tensordict\n",
    "    QValueModule(spec=env.action_spec) # reads the \"action_values\" entry by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(max_steps=3, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it relies on the argmax operator, this policy is determinstic. During data collection, we need to explore the environment. For that we can use the `EGreedyModule` once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_explore = TensorDictSequential(policy, EGreedyModule(env.action_spec))\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
